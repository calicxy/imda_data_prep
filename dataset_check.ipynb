{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.load_dataset('./loadingScript_imda_part3.py', \"Audio Same CloseMic\", split=\"train\", streaming=True)\n",
    "\n",
    "print(next(iter(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display\n",
    "\n",
    "index = 0\n",
    "for i in data:\n",
    "    if index == 1319: \n",
    "        \n",
    "        display_data = i['audio']\n",
    "\n",
    "        print(f'sampling rate: {display_data[\"sampling_rate\"]}')\n",
    "        display(Audio(display_data[\"array\"], rate=display_data[\"sampling_rate\"]))\n",
    "        print(f'transcript: {i[\"transcript\"]}')\n",
    "        # print(f'gender: {i[\"gender\"]}')\n",
    "        # print(f'race: {i[\"race\"]}')\n",
    "        print(f'interval: {i[\"interval\"]}')\n",
    "        break\n",
    "    index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "path_to_data = 'D:\\IMDA - National Speech Corpus\\PART1'\n",
    "mic = \"CHANNEL0\"\n",
    "speaker = '0001'\n",
    "metadata_path = os.path.join(path_to_data, \"DATA\", mic, \"SCRIPT\", mic[-1]+speaker+'*.TXT')\n",
    "print(metadata_path)\n",
    "script_list = glob.glob(metadata_path)\n",
    "print(script_list)\n",
    "d = {}\n",
    "for script in script_list:\n",
    "    line_num = 0\n",
    "    with open(script, encoding='utf-8-sig') as f:\n",
    "        for line in f:\n",
    "            if line_num == 0:\n",
    "                key = line.split(\"\\t\")[0]\n",
    "                line_num += 1\n",
    "\n",
    "            elif line_num == 1:\n",
    "                d[key] = line.strip()\n",
    "                line_num -= 1\n",
    "\n",
    "    break\n",
    "\n",
    "print(d)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_path = r'D:\\IMDA - National Speech Corpus\\PART1\\DATA\\CHANNEL0\\WAVE\\SPEAKER0001.zip'\n",
    "from datasets.download import DownloadManager\n",
    "dl_manager = DownloadManager()\n",
    "if os.path.exists(archive_path):\n",
    "    audio_files = dl_manager.iter_archive(archive_path)\n",
    "    for path, f in audio_files:\n",
    "        print(path)\n",
    "        break\n",
    "else:\n",
    "    print('error')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring speaker information for train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "p1_speaker_df = pd.read_excel(\"./Speaker Information (Part 1).XLSX\", dtype={'SCD/PART1': object})\n",
    "p2_speaker_df = pd.read_excel(\"./Speaker Information (Part 2).XLSX\", dtype={'SCD/PART2': object})\n",
    "p1_speaker_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1_speaker_df[\"SEX\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1_speaker_df[\"ACC\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = p1_speaker_df[(p1_speaker_df[\"ACC\"]==\"CHINESE\") & (p1_speaker_df[\"SEX\"]==\"F\")]\n",
    "print(\"number of rows: \", len(X))\n",
    "X_train, X_test = train_test_split(X, test_size=0.3, random_state=42, shuffle=True)\n",
    "\n",
    "print(\"number of rows in train: \",len(X_train))\n",
    "print(\"number of rows in test: \",len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genderlist = p1_speaker_df[\"SEX\"].unique()\n",
    "racelist = p1_speaker_df[\"ACC\"].unique()\n",
    "train_speaker_ids = []\n",
    "test_speaker_ids = []\n",
    "\n",
    "for gender in genderlist:\n",
    "    for race in racelist:\n",
    "        X = p1_speaker_df[(p1_speaker_df[\"ACC\"]==race) & (p1_speaker_df[\"SEX\"]==gender)]\n",
    "        X_train, X_test = train_test_split(X, test_size=0.3, random_state=42, shuffle=True)\n",
    "        train_speaker_ids.extend(X_train[\"SCD/PART1\"])\n",
    "        test_speaker_ids.extend(X_test[\"SCD/PART1\"])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1_speaker_df.loc[p1_speaker_df[\"SCD/PART1\"]==\"0001\"].iloc[0][\"SEX\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import cleanup_string\n",
    "\n",
    "line = \"<fil> you can go first  you guys are going to stand here <fil>\"\n",
    "print(cleanup_string(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = r'C:\\Users\\calic\\Downloads\\huggingface-dataset\\imda-dataset\\IMDA - National Speech Corpus\\PART3'\n",
    "mics = [\"Audio Same BoundaryMic\", \"Audio Same CloseMic\", \"Audio Separate IVR\", \"Audio Separate StandingMic\"]\n",
    "for mic in mics:\n",
    "    for (root,dirs,files) in os.walk(os.path.join(path_to_data, mic), topdown=True):\n",
    "        print(root)\n",
    "        # print(dirs)\n",
    "        # print(files)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (root,dirs,files) in os.walk(os.path.join(path_to_data, mics[2]), topdown=True):\n",
    "    if len(files) != 0:\n",
    "        for file in files:\n",
    "            print(\"path: \", os.path.join(root, file))\n",
    "            if mics[2].split()[1] == \"Same\":\n",
    "                print(\"script path: \", os.path.join(path_to_data, \"Scripts Same\", file[:-4]+\".TextGrid\"))\n",
    "            elif mics[2].split()[1] == \"Separate\":\n",
    "                if mics[2] == \"Audio Separate IVR\":\n",
    "                    print(\"script path: \", os.path.join(path_to_data, \"Scripts Separate\", os.path.split(root)[-1]+\"_\"+file[:-4]+\".TextGrid\"))\n",
    "                elif mics[2] == \"Audio Separate StandingMic\":\n",
    "                    print(\"script path: \", os.path.join(path_to_data, \"Scripts Separate\", file[:-4]+\".TextGrid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mics = [\"Audio Same BoundaryMic\", \"Audio Same CloseMic\", \"Audio Separate IVR\", \"Audio Separate StandingMic\"]\n",
    "mics=[\"Audio Same CloseMic\"]\n",
    "audio_list = []\n",
    "for mic in mics:\n",
    "    for (root,dirs,files) in os.walk(os.path.join(path_to_data, mic), topdown=True):\n",
    "        if len(files) != 0:\n",
    "            for file in files:\n",
    "                # get audio path\n",
    "                audio_path = os.path.join(root, file)\n",
    "                audio_list.append(audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textgrid import textgrid\n",
    "import soundfile as sf\n",
    "from clean_transcript import cleanup_string\n",
    "\n",
    "INTERVAL_MAX_LENGTH = 25\n",
    "\n",
    "id_=0\n",
    "for audio_path in audio_list:\n",
    "    print(\"AUDIO: \",audio_path)\n",
    "    file = os.path.split(audio_path)[-1]\n",
    "    folder = os.path.split(os.path.split(audio_path)[0])[-1]\n",
    "\n",
    "    # get script_path\n",
    "    if folder.split(\"_\")[0] == \"conf\":\n",
    "        # mic == \"Audio Separate IVR\"\n",
    "        script_path = os.path.join(path_to_data, \"Scripts Separate\", folder+\"_\"+file[:-4]+\".TextGrid\")\n",
    "    elif folder.split()[1] == \"Same\":\n",
    "        # mic == \"Audio Same CloseMic IVR\"\n",
    "        script_path = os.path.join(path_to_data, \"Scripts Same\", file[:-4]+\".TextGrid\")\n",
    "    elif folder.split()[1] == \"Separate\":\n",
    "        # mic == \"Audio Separate StandingMic\":\n",
    "        script_path = os.path.join(path_to_data, \"Scripts Separate\", file[:-4]+\".TextGrid\")\n",
    "\n",
    "    tg = textgrid.TextGrid.fromFile(script_path)\n",
    "    data, sr = sf.read(audio_path)\n",
    "    result = {}\n",
    "    i = 0\n",
    "    intervalLength = 0\n",
    "    intervalStart = 0\n",
    "    transcript_list = []\n",
    "    filepath = os.path.join(path_to_data, 'tmp_clip.wav')\n",
    "    while i < (len(tg[0])-1):\n",
    "        transcript = cleanup_string(tg[0][i].mark)\n",
    "        if intervalLength == 0 and len(transcript) == 0:\n",
    "            intervalStart = tg[0][i].maxTime\n",
    "            i+=1\n",
    "            continue\n",
    "        intervalLength += tg[0][i].maxTime-tg[0][i].minTime\n",
    "        if intervalLength > INTERVAL_MAX_LENGTH:\n",
    "            print(f\"INTERVAL LONGER THAN {intervalLength}\")\n",
    "            result[\"transcript\"] = transcript\n",
    "            result[\"interval\"] = \"start:\"+str(tg[0][i].minTime)+\", end:\"+str(tg[0][i].maxTime)\n",
    "            result[\"audio\"] = {\"path\": audio_path, \"bytes\": data[int(tg[0][i].minTime*sr):int(tg[0][i].maxTime*sr)], \"sampling_rate\":sr}\n",
    "            print( id_, result)\n",
    "            id_+= 1\n",
    "            intervalLength = 0\n",
    "        else:\n",
    "            if (intervalLength + tg[0][i+1].maxTime-tg[0][i+1].minTime) < INTERVAL_MAX_LENGTH:\n",
    "                if len(transcript) != 0:\n",
    "                    transcript_list.append(transcript)\n",
    "                i+=1\n",
    "                continue\n",
    "            if len(transcript) == 0:\n",
    "                spliced_audio = data[int(intervalStart*sr):int(tg[0][i].minTime*sr)]\n",
    "            else:\n",
    "                transcript_list.append(transcript)\n",
    "                spliced_audio = data[int(intervalStart*sr):int(tg[0][i].maxTime*sr)]\n",
    "            sf.write(filepath, spliced_audio, sr)\n",
    "            result[\"interval\"] = \"start:\"+str(intervalStart)+\", end:\"+str(tg[0][i].maxTime)\n",
    "            result[\"audio\"] = {\"path\": filepath, \"bytes\": spliced_audio, \"sampling_rate\":sr}\n",
    "            result[\"transcript\"] = ' '.join(transcript_list)\n",
    "            print(id_, result)\n",
    "            id_+= 1\n",
    "            intervalLength=0\n",
    "            intervalStart=tg[0][i].maxTime\n",
    "            transcript_list = []\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "83ff679c0ae51c3298408e270a36a6da442d58608875fa78406d64401a44cc70"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
